{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queryTokens': [{'text': 'chimp', 'type': 'TERM'}],\n",
       " 'ngrams': [{'id': '1b10fd2d586ef5c52a322f66c0f5179a',\n",
       "   'absTotalMatchCount': 251006,\n",
       "   'relTotalMatchCount': 1.2565909557086897e-07,\n",
       "   'tokens': [{'text': 'chimp', 'type': 'TERM'}]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "ngrams_api = \"https://api.ngrams.dev\"\n",
    "corpus_id = \"eng\"\n",
    "data = {\"query\":\"chimp\", \"flags\": \"cs\"}\n",
    "r = requests.get(f\"{ngrams_api}/{corpus_id}/search\", params=data)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chimp', 'chimpanzee') longer word more popular now\n",
      "('info', 'information') longer word more popular now\n",
      "('dorm', 'dormitory') shorter word more popular now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chimp': {'start_year': 1706,\n",
       "  'start_rel_count': 7.728846706904589e-08,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 2.025308493964661e-07,\n",
       "  'max_slope': 5.3373676134432925e-08,\n",
       "  'max_slope_year': 1942},\n",
       " 'chimpanzee': {'start_year': 1523,\n",
       "  'start_rel_count': 2.0853796120776845e-06,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 6.629676279292064e-07,\n",
       "  'max_slope': 2.0622884925804945e-06,\n",
       "  'max_slope_year': 1523},\n",
       " 'info': {'start_year': 1501,\n",
       "  'start_rel_count': 3.2394508094253764e-07,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 2.9698391262354393e-06,\n",
       "  'max_slope': 7.53557623768214e-05,\n",
       "  'max_slope_year': 1554},\n",
       " 'information': {'start_year': 1472,\n",
       "  'start_rel_count': 0.0005524767959745691,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 0.0002847724370683589,\n",
       "  'max_slope': 0.0014490783769568402,\n",
       "  'max_slope_year': 1506},\n",
       " 'dorm': {'start_year': 1616,\n",
       "  'start_rel_count': 1.118673342909177e-06,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 2.1492890917998327e-06,\n",
       "  'max_slope': 9.576895549816965e-07,\n",
       "  'max_slope_year': 1631},\n",
       " 'dormitory': {'start_year': 1501,\n",
       "  'start_rel_count': 2.2676155665977635e-06,\n",
       "  'end_year': 2019,\n",
       "  'final_rel_count': 1.3471390047461242e-06,\n",
       "  'max_slope': 7.153299785972647e-06,\n",
       "  'max_slope_year': 1594}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_pairs = [(\"chimp\", \"chimpanzee\"), (\"info\", \"information\"), (\"dorm\", \"dormitory\")]\n",
    "word_data = {}\n",
    "for wp in word_pairs:\n",
    "    for idx, word in enumerate(wp):\n",
    "        data = {\"query\": word, \"flags\": \"cs\"}\n",
    "        request_id = requests.get(f\"{ngrams_api}/{corpus_id}/search\", params=data)\n",
    "        id = request_id.json()[\"ngrams\"][0][\"id\"]\n",
    "        ngram_request = requests.get(f\"{ngrams_api}/{corpus_id}/{id}\", params=data)\n",
    "        ngram_request = ngram_request.json()\n",
    "        word_data[word] = {\"start_year\":ngram_request[\"stats\"][0][\"year\"]}\n",
    "        word_data[word][\"start_rel_count\"] = ngram_request[\"stats\"][0][\"relMatchCount\"]\n",
    "        word_data[word][\"end_year\"] = ngram_request[\"stats\"][-1][\"year\"]\n",
    "        word_data[word][\"final_rel_count\"] = ngram_request[\"stats\"][-1][\"relMatchCount\"]\n",
    "        points = [ngram_request[\"stats\"][i][\"relMatchCount\"] for i in range(len(ngram_request[\"stats\"]))]\n",
    "        max_slope = max([x - z for x, z in zip(points[:-1], points[1:])])\n",
    "        max_slope_id = np.argmax([x - z for x, z in zip(points[:-1], points[1:])])\n",
    "        word_data[word][\"max_slope\"] = max_slope\n",
    "        word_data[word][\"max_slope_year\"] = ngram_request[\"stats\"][max_slope_id][\"year\"]\n",
    "        # TODO find maybe decade with biggest increase\n",
    "for wp in word_pairs: \n",
    "    if word_data[wp[0]][\"final_rel_count\"]> word_data[wp[1]][\"final_rel_count\"]:\n",
    "        print(wp, \"shorter word more popular now\")\n",
    "    else:\n",
    "        print(wp, \"longer word more popular now\")\n",
    "word_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
